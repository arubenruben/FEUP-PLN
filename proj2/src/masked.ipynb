{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AutoConfig\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import math\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "from datasets import load_metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELS = 5\n",
    "chunk_size = 128\n",
    "\n",
    "model_name = \"neuralmind/bert-base-portuguese-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODING = {\"label\": {\"Value\": 0, \"Value(+)\": 1, \"Value(-)\": 2, \"Fact\": 3, \"Policy\": 4}}\n",
    "\n",
    "\n",
    "def create_index_column(df):\n",
    "    df[\"id\"] = df.index + 1\n",
    "\n",
    "def load_dataset():\n",
    "    \n",
    "    df_text = pd.DataFrame(pd.read_excel(os.path.join('./dataset', 'OpArticles.ods')))\n",
    "\n",
    "    df_adu = pd.DataFrame(\n",
    "        pd.read_excel(os.path.join('./dataset', 'aug.ods')))\n",
    "    \n",
    "    create_index_column(df_adu)\n",
    "    \n",
    "    return df_adu, df_text\n",
    "\n",
    "\n",
    "\n",
    "def remove_dataframe_rows_by_id(df_to_remove, list_ids_to_remove):\n",
    "    df_to_remove.set_index(\"id\", inplace=True)\n",
    "\n",
    "    df_to_remove.drop(list_ids_to_remove, inplace=True)\n",
    "\n",
    "    df_to_remove.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "def split_train_test(df):\n",
    "  \n",
    "    train, test = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "    \n",
    "    val, test = train_test_split(test, test_size=0.5, random_state=42, stratify=test['label'])\n",
    "\n",
    "    train = pd.DataFrame.from_dict(train)\n",
    "    \n",
    "    val = pd.DataFrame.from_dict(val)\n",
    "    \n",
    "    test = pd.DataFrame.from_dict(test)\n",
    "    \n",
    "    return train, val, test\n",
    "    \n",
    "def outlier_detection(df_adu):\n",
    "    results = {}\n",
    "    dict_collisions = {}\n",
    "    for _, row in df_adu.iterrows():\n",
    "        if row['article_id'] not in results.keys():\n",
    "            results[row['article_id']] = {\n",
    "                'A': [],\n",
    "                'B': [],\n",
    "                'C': [],\n",
    "                'D': []\n",
    "            }\n",
    "\n",
    "        if row['annotator'] == 'A':\n",
    "            results[row['article_id']]['A'].append({\n",
    "                'id': row['id'],\n",
    "                'ranges': row['ranges'],\n",
    "                'tokens': row['tokens'],\n",
    "                'label': row['label']\n",
    "            })\n",
    "\n",
    "        elif row['annotator'] == 'B':\n",
    "            results[row['article_id']]['B'].append({\n",
    "                'id': row['id'],\n",
    "                'ranges': row['ranges'],\n",
    "                'tokens': row['tokens'],\n",
    "                'label': row['label']\n",
    "            })\n",
    "\n",
    "        elif row['annotator'] == 'C':\n",
    "            results[row['article_id']]['C'].append({\n",
    "                'id': row['id'],\n",
    "                'ranges': row['ranges'],\n",
    "                'tokens': row['tokens'],\n",
    "                'label': row['label']\n",
    "            })\n",
    "\n",
    "        elif row['annotator'] == 'D':\n",
    "            results[row['article_id']]['D'].append({\n",
    "                'id': row['id'],\n",
    "                'ranges': row['ranges'],\n",
    "                'tokens': row['tokens'],\n",
    "                'label': row['label']\n",
    "            })\n",
    "\n",
    "    for article_id in results.keys():\n",
    "\n",
    "        for adu_A in results[article_id]['A']:\n",
    "            adu_matching(adu_A, results[article_id]['B'], results[article_id]['C'], results[article_id]['D'],\n",
    "                         dict_collisions)\n",
    "        for adu_B in results[article_id]['B']:\n",
    "            adu_matching(adu_B, results[article_id]['A'], results[article_id]['C'], results[article_id]['D'],\n",
    "                         dict_collisions)\n",
    "        for adu_C in results[article_id]['C']:\n",
    "            adu_matching(adu_C, results[article_id]['A'], results[article_id]['B'], results[article_id]['D'],\n",
    "                         dict_collisions)\n",
    "        for adu_D in results[article_id]['D']:\n",
    "            adu_matching(adu_D, results[article_id]['A'], results[article_id]['B'], results[article_id]['C'],\n",
    "                         dict_collisions)\n",
    "\n",
    "    return dict_collisions\n",
    "\n",
    "\n",
    "def adu_matching(adu, list_annotater_X, list_annotater_Y, list_annotater_Z, dict_collisions):\n",
    "    for iterator in [list_annotater_X, list_annotater_Y, list_annotater_Z]:\n",
    "        for elem in iterator:\n",
    "            if json.loads(adu['ranges'])[0][0] < json.loads(elem['ranges'])[0][0] < json.loads(adu['ranges'])[0][1]:\n",
    "                if adu['label'] != elem['label']:\n",
    "                    # print(f\"Disagreement between:\\n{adu['tokens']} \\n and \\n {elem['tokens']}\")\n",
    "                    if adu['id'] not in dict_collisions.keys():\n",
    "                        dict_collisions[adu['id']] = [elem['id']]\n",
    "                    else:\n",
    "                        dict_collisions[adu['id']].append(elem['id'])\n",
    "\n",
    "\n",
    "def deal_with_outliers(df_adu, dict_collisions, option='delete'):\n",
    "    # print(f\"Before:{df_adu.describe()}\")\n",
    "\n",
    "    if option == 'delete':\n",
    "        list_to_remove = []\n",
    "\n",
    "        for key_left in dict_collisions.keys():\n",
    "            list_to_remove.append(key_left)\n",
    "            for elem in dict_collisions[key_left]:\n",
    "                list_to_remove.append(elem)\n",
    "\n",
    "        remove_dataframe_rows_by_id(df_adu, list_to_remove)\n",
    "\n",
    "    elif option == 'majority':\n",
    "        list_to_remove = []\n",
    "        for key_left in dict_collisions.keys():\n",
    "            counters = {\n",
    "                'Fact': 0,\n",
    "                'Policy': 0,\n",
    "                'Value': 0,\n",
    "                'Value(+)': 0,\n",
    "                'Value(-)': 0,\n",
    "            }\n",
    "\n",
    "            majority_vote = None\n",
    "            number_votes = 0\n",
    "\n",
    "            adu = df_adu.loc[df_adu['id'] == key_left].iloc[0]\n",
    "\n",
    "            counters[adu['label']] += 1\n",
    "\n",
    "            for elem in dict_collisions[key_left]:\n",
    "                adu = df_adu.loc[df_adu['id'] == elem].iloc[0]\n",
    "                counters[adu['label']] += 1\n",
    "\n",
    "            for elem in counters.keys():\n",
    "                number_votes += counters[elem]\n",
    "\n",
    "            \"\"\"\n",
    "            Find the majority vote type\n",
    "            Majority_Vote returns a Valid Label\n",
    "            \"\"\"\n",
    "\n",
    "            for elem in counters.keys():\n",
    "                if counters[elem] / number_votes >= 0.5:\n",
    "                    majority_vote = elem\n",
    "                    break\n",
    "\n",
    "            if not majority_vote:\n",
    "                continue\n",
    "\n",
    "            if adu['label'] != majority_vote:\n",
    "                list_to_remove.append(adu['id'])\n",
    "\n",
    "            for elem in dict_collisions[key_left]:\n",
    "                elem_adu = df_adu.loc[df_adu['id'] == elem].iloc[0]\n",
    "                if elem_adu['label'] != majority_vote:\n",
    "                    list_to_remove.append(elem_adu['id'])\n",
    "\n",
    "        remove_dataframe_rows_by_id(df_adu, list_to_remove)\n",
    "\n",
    "    # print(f\"After:{df_adu.describe()}\")\n",
    "#ENCODING = {\"label\": {\"Value\": 0, \"Value(+)\": 1, \"Value(-)\": 2, \"Fact\": 3, \"Policy\": 4}}   \n",
    "def augment_train(df_train):\n",
    "    new_lines={\n",
    "        'tokens':[],\n",
    "        'label':[],\n",
    "    }\n",
    "    for _, row in df_train.iterrows():\n",
    "        #if row['label'] == 1 or row['label'] == 4:\n",
    "        if row['label'] == 4 :\n",
    "            correct_str = row['augmented']\n",
    "            \n",
    "            en_str = row['en']\n",
    "            sp_str = row['sp']\n",
    "            \n",
    "            if not row['tokens'][0].isupper():\n",
    "                correct_str = correct_str[0].lower() + correct_str[1:]\n",
    "                en_str = en_str[0].lower() + en_str[1:]\n",
    "                sp_str = sp_str[0].lower() + sp_str[1:]\n",
    "                        \n",
    "            new_lines['tokens'].append(correct_str)\n",
    "            new_lines['label'].append(row['label'])\n",
    "            #new_lines['tokens'].append(en_str)\n",
    "            #new_lines['label'].append(row['label'])\n",
    "            #new_lines['tokens'].append(sp_str)\n",
    "            #new_lines['label'].append(row['label'])\n",
    "        \n",
    "    df = pd.DataFrame(new_lines)\n",
    "    \n",
    "    return df_train.append(df, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_for_masking(df):\n",
    "    \n",
    "    df.drop(columns=['article_id', 'title', 'authors', 'meta_description', 'topics', 'keywords', 'publish_date',\n",
    "                     'url_canonical'], inplace=True)\n",
    "    \n",
    "    df.rename(columns={'body': 'tokens'}, inplace=True)\n",
    "\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "\n",
    "    train_test = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "    \n",
    "    train_valid_test_dataset = DatasetDict({\n",
    "        'train': train_test['train'],\n",
    "        'test': train_test['test'],        \n",
    "    })\n",
    "\n",
    "    return train_valid_test_dataset\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function_2(examples):\n",
    "    result = tokenizer(examples[\"tokens\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"tokens\"], padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adu, _ = load_dataset()\n",
    "\n",
    "train, val, test = split_train_test(df_adu)\n",
    "\n",
    "dict_collisions = outlier_detection(train)\n",
    "\n",
    "deal_with_outliers(train, dict_collisions, 'delete')\n",
    "\n",
    "train.drop(columns=['article_id', 'annotator', 'node', 'ranges','id'], inplace = True)\n",
    "train.replace(ENCODING, inplace=True)\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "val.drop(columns=['article_id', 'annotator', 'node', 'ranges','id'], inplace = True)\n",
    "val.replace(ENCODING, inplace=True)\n",
    "val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "test.drop(columns=['article_id', 'annotator', 'node', 'ranges','id'], inplace = True)\n",
    "test.replace(ENCODING, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train = augment_train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_2():\n",
    "    \n",
    "    _, df_text = load_dataset()\n",
    "\n",
    "    \n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_name, num_labels = NUM_LABELS)\n",
    "    \n",
    "    dataset = load_data_for_masking(df_text)\n",
    "\n",
    "    tokenized_datasets = dataset.map(\n",
    "        tokenize_function_2, batched=True, remove_columns=[\"tokens\"]\n",
    "    )\n",
    "\n",
    "    lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "    \n",
    "    max_layer = 11\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        for elem in range(max_layer-2):\n",
    "            if name.startswith(f\"bert.encoder.layer.{elem}.\"): # choose whatever you like here\n",
    "                param.requires_grad = False\n",
    "    #batch_size = 64\n",
    "    # Show the training loss with every epoch\n",
    "    #logging_steps = len(lm_datasets[\"train\"]) // batch_size\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{model_name}-finetuned-imdb\",        \n",
    "        evaluation_strategy=\"epoch\",\n",
    "        num_train_epochs=3,\n",
    "        fp16=True,\n",
    "        save_strategy='no'\n",
    "    )\n",
    "\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=lm_datasets[\"train\"],\n",
    "        eval_dataset=lm_datasets[\"test\"],\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\">>> Perplexity Before: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\">>> Perplexity After: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(f\"bert.encoder.layer.{elem}.\"): # choose whatever you like here\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    #batch_size = 64\n",
    "    # Show the training loss with every epoch\n",
    "    #logging_steps = len(lm_datasets[\"train\"]) // batch_size\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{model_name}-finetuned-imdb\",        \n",
    "        evaluation_strategy=\"epoch\",\n",
    "        num_train_epochs=2,\n",
    "        fp16=True,\n",
    "        save_strategy='no'\n",
    "    )\n",
    "\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=lm_datasets[\"train\"],\n",
    "        eval_dataset=lm_datasets[\"test\"],\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\">>> Perplexity Before: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\">>> Perplexity After: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "    \n",
    "    \n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    config.num_labels = NUM_LABELS\n",
    "    model = AutoModelForSequenceClassification.from_config(config)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f89729b7eed4f03ad2aa346756b630c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9942fa5bb64b4c1f82627908cb05c344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72855dce311e41ea8ac08bc1bf09e375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e977b772bb4440a3b1af3c4cbbf3b7d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/setuptools/distutils_patch.py:26: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  \"Distutils was imported before Setuptools. This usage is discouraged \"\n",
      "Using amp half precision backend\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 597\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 00:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 2662\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity Before: 11.08\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='999' max='999' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [999/999 01:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.888990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.059500</td>\n",
       "      <td>1.802909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.059500</td>\n",
       "      <td>1.801823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 597\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 597\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 597\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 597\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 597\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity After: 6.22\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 00:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 2662\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity Before: 5.98\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='666' max='666' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [666/666 01:17, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.972661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.328300</td>\n",
       "      <td>1.874308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 597\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 597\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 597\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/neuralmind/bert-base-portuguese-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e716e2151985ba669e7197b64cdde2552acee146494d40ffaf0688a3f152e6ed.18a0b8b86f3ebd4c8a1d8d6199178feae9971ff5420f1d12f0ed8326ffdff716\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"neuralmind/bert-base-portuguese-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 29794\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity After: 6.67\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "067e3747b9f141f4b4ab1880de4d3f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec56cfbaae1840e9a729c9c8ffa0e2f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7147930659744f29f07c8218659b475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_valid_test_dataset = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train),\n",
    "    'validation': Dataset.from_pandas(val),\n",
    "    'test': Dataset.from_pandas(test)\n",
    "})\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=False)\n",
    "\n",
    "model = task_2()\n",
    "\n",
    "tokenized_dataset = train_valid_test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "max_layer = 11\n",
    "#max_layer = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "#ENCODING = {\"label\": {\"Value\": 0, \"Value(+)\": 1, \"Value(-)\": 2, \"Fact\": 3, \"Policy\": 4}}\n",
    "from torch import nn\n",
    "from transformers import Trainer\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        # compute custom loss\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        #loss_fct = nn.CrossEntropyLoss(weight=torch.cuda.FloatTensor([0.11, 0.92, 0.52, 0.485, 0.97]))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    metric = load_metric(\"f1\")\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    f1_val = metric.compute(predictions=predictions, references=labels, average='macro')\n",
    "\n",
    "    metric = load_metric(\"accuracy\")\n",
    "    acc_val = metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    return {\n",
    "        'f1':f1_val['f1'],\n",
    "        'accuracy':acc_val['accuracy'],\n",
    "    }\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    for elem in range(max_layer-5):\n",
    "        if name.startswith(f\"bert.encoder.layer.{elem}.\"): # choose whatever you like here\n",
    "            param.requires_grad = False\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=7,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\", # run validation at the end of each epoch\n",
    "    save_strategy=\"no\",\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens, augmented, en, sp. If tokens, augmented, en, sp are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 12390\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1358\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1358' max='1358' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1358/1358 14:07, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.397000</td>\n",
       "      <td>1.340892</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.483871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.231300</td>\n",
       "      <td>1.289472</td>\n",
       "      <td>0.323330</td>\n",
       "      <td>0.438471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.014000</td>\n",
       "      <td>1.244484</td>\n",
       "      <td>0.402697</td>\n",
       "      <td>0.496416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.856500</td>\n",
       "      <td>1.249048</td>\n",
       "      <td>0.442719</td>\n",
       "      <td>0.523297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.747900</td>\n",
       "      <td>1.330831</td>\n",
       "      <td>0.450050</td>\n",
       "      <td>0.502389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.660900</td>\n",
       "      <td>1.396428</td>\n",
       "      <td>0.448658</td>\n",
       "      <td>0.503584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.604100</td>\n",
       "      <td>1.461649</td>\n",
       "      <td>0.444599</td>\n",
       "      <td>0.495221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens, augmented, en, sp. If tokens, augmented, en, sp are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1674\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens, augmented, en, sp. If tokens, augmented, en, sp are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1674\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens, augmented, en, sp. If tokens, augmented, en, sp are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1674\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens, augmented, en, sp. If tokens, augmented, en, sp are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1674\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens, augmented, en, sp. If tokens, augmented, en, sp are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1674\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens, augmented, en, sp. If tokens, augmented, en, sp are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1674\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens, augmented, en, sp. If tokens, augmented, en, sp are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1674\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens, augmented, en, sp. If tokens, augmented, en, sp are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1674\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='54' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.461648941040039,\n",
       " 'eval_f1': 0.4445994720666321,\n",
       " 'eval_accuracy': 0.49522102747909197,\n",
       " 'eval_runtime': 4.1574,\n",
       " 'eval_samples_per_second': 402.66,\n",
       " 'eval_steps_per_second': 6.495,\n",
       " 'epoch': 7.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens, augmented, en, sp. If tokens, augmented, en, sp are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1675\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "predictions=trainer.predict(test_dataset=tokenized_dataset[\"test\"])\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "confusion_matrix = metrics.confusion_matrix(tokenized_dataset['test']['label'], preds, labels=[0,1,2,3,4])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix,display_labels=[\"Value\",\"Value(+)\",\"Value(-)\",\"Fact\",\"Policy\"])\n",
    "disp.plot()\n",
    "#print(confusion_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!pip install xlrd\\n!pip install odfpy\\n!pip install ipywidgets widgetsnbextension pandas-profiling\\n!pip install datasets\\n!pip install transformers\\n!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu110\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "!pip install xlrd\n",
    "!pip install odfpy\n",
    "!pip install ipywidgets widgetsnbextension pandas-profiling\n",
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu110\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "#!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
