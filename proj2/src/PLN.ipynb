{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PLN.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# constants.py"
      ],
      "metadata": {
        "id": "-adBCOqVz6TS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_LABELS = 5"
      ],
      "metadata": {
        "id": "WCAqKRElz6rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_loading.py"
      ],
      "metadata": {
        "id": "AZOmP3QutjWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from datasets import DatasetDict\n",
        "\n",
        "ENCODING = {\"label\": {\"Value\": 0, \"Value(+)\": 1, \"Value(-)\": 2, \"Fact\": 3, \"Policy\": 4}}\n",
        "\n",
        "\n",
        "def load_dataset():\n",
        "    \n",
        "    df_text = pd.DataFrame(pd.read_csv(os.path.join('/content','drive','Shareddrives','PLN','dataset', 'OpArticles.csv')))\n",
        "\n",
        "    df_adu = pd.DataFrame(\n",
        "        pd.read_csv(os.path.join('/content','drive','Shareddrives','PLN','dataset', 'OpArticles_ADUs.csv')))\n",
        "\n",
        "    return df_adu, df_text\n",
        "\n",
        "\n",
        "def normalize_dataset(df):\n",
        "    df.drop(columns=['article_id', 'annotator', 'node', 'ranges'], inplace=True)\n",
        "    df.replace(ENCODING, inplace=True)\n",
        "\n",
        "    dataset_hf = Dataset.from_pandas(df)\n",
        "\n",
        "    return dataset_hf\n",
        "\n",
        "\n",
        "def split_train_test(df, test_percentage=0.2, validation_percentage=0.5):\n",
        "    dataset = normalize_dataset(df)\n",
        "\n",
        "    if test_percentage == 1.0:\n",
        "        return DatasetDict({\n",
        "            'test': dataset\n",
        "        })\n",
        "\n",
        "    train_test = dataset.train_test_split(test_size=test_percentage)\n",
        "\n",
        "    # Split the 10% test+validation set in half test, half validation\n",
        "    valid_test = train_test['test'].train_test_split(test_size=(1.0 - validation_percentage))\n",
        "\n",
        "    train_valid_test_dataset = DatasetDict({\n",
        "        'train': train_test['train'],\n",
        "        'validation': valid_test['train'],\n",
        "        'test': valid_test['test']\n",
        "    })\n",
        "\n",
        "    return train_valid_test_dataset\n"
      ],
      "metadata": {
        "id": "AiuUbLQBtlXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate.py"
      ],
      "metadata": {
        "id": "Y0KI6E_OtT_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    metric = load_metric(\"accuracy\")\n",
        "\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "def evaluate(y_test, y_pred):\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "    print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
        "    print('Precision: ', precision_score(y_test, y_pred, average='macro'))\n",
        "    print('Recall: ', recall_score(y_test, y_pred, average='macro'))\n",
        "    print('F1: ', f1_score(y_test, y_pred, average='macro'))\n"
      ],
      "metadata": {
        "id": "gIUUvYLNtTnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# main.py"
      ],
      "metadata": {
        "id": "C1uOJ1HdtKec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, DataCollatorWithPadding, Trainer\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, DataCollatorWithPadding, Trainer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "model_name = 'neuralmind/bert-base-portuguese-cased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=False)\n",
        "\n",
        "\n",
        "def task_1():\n",
        "    df_adu, _ = load_dataset()\n",
        "\n",
        "    dataset = split_train_test(df_adu)\n",
        "\n",
        "    tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=NUM_LABELS)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        num_train_epochs=3,\n",
        "        weight_decay=0.01,\n",
        "        evaluation_strategy=\"epoch\",  # run validation at the end of each epoch\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "    )\n",
        "\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset[\"train\"],\n",
        "        eval_dataset=tokenized_dataset[\"validation\"],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.evaluate()\n",
        "    trainer.predict(test_dataset=tokenized_dataset[\"test\"])\n",
        "\n",
        "\n",
        "def preprocess_function(sample):\n",
        "    return tokenizer(sample[\"tokens\"], truncation=True)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    task_1()\n"
      ],
      "metadata": {
        "id": "TijL6bhr2lkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gL454-TMyfmT"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "pgSXDwHktor7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5masSgQRuCbj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}